{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30804,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. Import Libraries and NLTK Setup\n- Import pandas and numpy for data manipulation.\n- Import scikit-learn libraries for text processing and similarity computation.\n- Import nltk for natural language processing.\n- Download required NLTK datasets for stopwords, tokenization, and lemmatization.","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport re\nimport string\nimport os\nimport pickle\nimport gc\n\n# Download required NLTK datasets\nnltk.download('punkt', quiet=True)\nnltk.download('stopwords', quiet=True)\nnltk.download('wordnet', quiet=True)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Define BookRecommender Class\n- The main class for building the book recommendation system.\n- Initializes text preprocessing tools, TF-IDF vectorizer, and placeholders for the dataset and similarity matrix.","metadata":{}},{"cell_type":"code","source":"class BookRecommender:\n    def __init__(self):\n        # Initialize tools for text preprocessing and recommendation\n        self.stop_words = set(stopwords.words('english'))\n        self.lemmatizer = WordNetLemmatizer()\n        self.vectorizer = TfidfVectorizer(max_features=5000)\n        self.df = None  # To store the dataset\n        self.tfidf_matrix = None  # TF-IDF matrix for recommendations","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Create Text Cleaning Method\n- `_clean_text` processes the text data by:\n   1. Converting text to lowercase.\n   2. Removing numbers, punctuation, and stop words.\n   3. Lemmatizing words for standardization.","metadata":{}},{"cell_type":"code","source":"    def _clean_text(self, text):\n        # Convert text to lowercase\n        text = str(text).lower()\n        # Remove numbers and unwanted characters\n        text = re.sub(r'\\([^)]*\\)|\\d+', '', text)\n        # Remove punctuation\n        text = text.translate(str.maketrans('', '', string.punctuation))\n        # Tokenize, remove stop words, and lemmatize\n        tokens = [\n            self.lemmatizer.lemmatize(word)\n            for word in nltk.word_tokenize(text)\n            if word not in self.stop_words and len(word) > 2\n        ]\n        # Join the tokens back into a single string\n        return ' '.join(tokens)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Fit the Recommender Model\n- The `fit` method:\n   1. Prepares the dataset by handling missing values.\n   2. Cleans and combines text columns (Title, Description, Category).\n   3. Creates a TF-IDF matrix for similarity computations.","metadata":{}},{"cell_type":"code","source":"    def fit(self, df):\n        print(\"Processing dataset...\")\n        # Make a copy of the dataset\n        self.df = df.copy()\n        \n        # Fill missing values in important columns\n        self.df['Description'] = self.df['Description'].fillna('')\n        self.df['Category'] = self.df['Category'].fillna('')\n        \n        # Combine and clean text data\n        print(\"Cleaning text...\")\n        self.df['processed_content'] = (\n            self.df['Title'].fillna('') + ' ' +\n            self.df['Description'] + ' ' +\n            self.df['Category']\n        ).apply(self._clean_text)\n        \n        # Create the TF-IDF matrix\n        print(\"Creating TF-IDF matrix...\")\n        self.tfidf_matrix = self.vectorizer.fit_transform(self.df['processed_content'])\n        \n        # Clean up unused memory\n        gc.collect()\n        print(\"Model ready!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Generate Book Recommendations\n - The `get_recommendations` method:\n   1. Finds the index of the given book in the dataset.\n   2. Computes similarity scores with other books using cosine similarity.\n   3. Returns the top N similar books with their details.","metadata":{}},{"cell_type":"code","source":"    def get_recommendations(self, book_title, n=5):\n        # Find the index of the book based on title\n        idx = self.df[self.df['Title'].str.contains(book_title, case=False, na=False)].index\n        if len(idx) == 0:\n            return f\"Book '{book_title}' not found.\"\n        \n        idx = idx[0]\n        \n        # Compute cosine similarity scores\n        sim_scores = cosine_similarity(\n            self.tfidf_matrix[idx:idx+1], \n            self.tfidf_matrix\n        ).flatten()\n        \n        # Exclude the input book and sort scores\n        sim_scores[idx] = -1\n        top_indices = sim_scores.argsort()[::-1][:n]\n        \n        # Retrieve details of the top recommendations\n        recommendations = []\n        for i in top_indices:\n            recommendations.append({\n                'Title': self.df.iloc[i]['Title'],\n                'Author': self.df.iloc[i]['Authors'],\n                'Category': self.df.iloc[i]['Category'],\n                'Similarity Score': round(float(sim_scores[i]), 3)\n            })\n        return recommendations\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Save and Load the Model\n- `save_model`: Saves the trained model components to a file.\n- `load_model`: Loads a previously saved model.","metadata":{}},{"cell_type":"code","source":"    def save_model(self, filepath):\n        # Ensure the directory exists\n        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n        # Save the model components\n        with open(filepath, 'wb') as f:\n            pickle.dump({\n                'vectorizer': self.vectorizer,\n                'df': self.df,\n                'tfidf_matrix': self.tfidf_matrix\n            }, f)\n        print(f\"Model saved to {filepath}\")\n\n    @staticmethod\n    def load_model(filepath):\n        # Load model components from a file\n        with open(filepath, 'rb') as f:\n            data = pickle.load(f)\n        \n        recommender = BookRecommender()\n        recommender.vectorizer = data['vectorizer']\n        recommender.df = data['df']\n        recommender.tfidf_matrix = data['tfidf_matrix']\n        \n        return recommender\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. Main Execution for Testing\n- This section demonstrates:\n   1. Loading the dataset and training the recommender system.\n   2. Saving the trained model for future use.\n   3. Testing recommendations for sample books.","metadata":{}},{"cell_type":"code","source":"# Load dataset\nprint(\"Loading dataset...\")\ndf = pd.read_csv('./dataset/book.csv')\n\n# Create and train the recommender system\nprint(\"Creating recommender system...\")\nrecommender = BookRecommender()\nrecommender.fit(df)\n\n# Save the model\nos.makedirs('./ml_model', exist_ok=True)\nrecommender.save_model('./ml_model/book_recommender.pkl')\n\n# Test recommendations\ntest_books = ['The Hobbit', 'Harry Potter and the Sorcerer\\'s Stone', '1984']\n\nprint(\"\\nTesting recommendations:\")\nfor book in test_books:\n    print(f\"\\nRecommendations for '{book}':\")\n    recommendations = recommender.get_recommendations(book)\n    \n    if isinstance(recommendations, str):\n        print(recommendations)\n    else:\n        for i, rec in enumerate(recommendations, 1):\n            print(f\"{i}. {rec['Title']} by {rec['Author']}\")\n            print(f\"   Category: {rec['Category']}\")\n            print(f\"   Similarity Score: {rec['Similarity Score']}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}